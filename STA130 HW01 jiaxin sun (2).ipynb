{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96346fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset from seaborn's built-in dataset\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the Titanic dataset:\\n\", titanic.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = titanic.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f'Number of rows: {villagers_df.shape[0]}')\n",
    "print(f'Number of columns: {villagers_df.shape[1]}')\n",
    "\n",
    "# Observations: an observation refers to row in the DataFrame, where each row represents a unique villager with various attributes, such as name, gender, species.\n",
    "# Variables: Variables are the columns in the dataset, representing the features of villagers (e.g., gender, species, personality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Summarize each column\n",
    "for column in villagers_df.columns:\n",
    "    print(f\"Summary of '{column}':\")\n",
    "    print(villagers_df[column].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731bdc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_n           0\n",
      "id              1\n",
      "name            0\n",
      "gender          0\n",
      "species         0\n",
      "birthday        0\n",
      "personality     0\n",
      "song           11\n",
      "phrase          0\n",
      "full_id         0\n",
      "url             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = villagers_df.isnull().sum()\n",
    "\n",
    "# Show which columns have missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "\n",
    "#（1）.About the discrepancies between df.shape and df.describe:\n",
    "#df.shape: This function returns the overall dimensions of the dataset, specifically the number of rows (observations) and columns (variables). \n",
    "#For example, if the dataset contains 300 rows and 10 columns, will return , regardless of whether the columns are numeric or non-numeric.df.shape(300, 10)\n",
    "#df.describe(): This function provides summary statistics for numeric columns only by default. It will ignore non-numeric (categorical) columns\n",
    "#unless explicitly told to include them using . It summarizes only the numeric columns by providing information like the count, mean, standard deviation, etc.df.describe(include='all')\n",
    "\n",
    "#（2）.About the number of Columns Analyzed:\n",
    "#df.shape reflects all columns in the dataset (numeric and non-numeric).\n",
    "#df.describe() only includes the numeric columns, hence fewer columns are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "#(1)Attribute: attribute is a kind of data related to an object. it shows the feature of the object and it does not need parentheses when we use it. \n",
    "#An attribute provide information more easier to get and understand. It is accessed directly like a variable.\n",
    "\n",
    "#(2)Method: method is a kind of function which use to execute operations and computations. \n",
    "#It requires parentheses when we call it, in order to help it process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d20eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "#'count': The number of non-null observations in the dataset for each variable. It tells us how many data points are available.\n",
    "#'mean': the average value of the observations for each variable. \n",
    "#'std': the whole name of std is standard deviation.If it shows as a axes, the more concentrated the point distribution is,the std will be more higher. In conclusion, a higher standard deviation indicates more variability.\n",
    "#'min': it is the smallest value in the dataset, represent the minimum limit of the data range.\n",
    "#'25%': it represents the value below which 25% of the data points are found. It shows where the bottom quarter of the data lies.\n",
    "#'50%',This is the middle value of the dataset. Half of the data points are below this value, and half are above it. It gives an idea of the center of the data.\n",
    "#'75%': This is the value below which 75% of the data points are found. It shows where the top quarter of the data lies.\n",
    "#'max': This is the highest value in the dataset. It marks the upper end of the data range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a52c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "#(1)Use case: Assume I have a dataset of students' grades (df) with columns: student_name, math_score,and english_score. \n",
    "#The english_score column has many missing values. I want to analyze the data, when I use df.dropna() to remove only the rows that have missing values. \n",
    "#This is useful, rather than using del df['col']. because removing a few rows with missing values keeps the majority of the data. I will not lose other students data.\n",
    "\n",
    "#(2)use case: Assume I have a dataset of students' grades (df) with columns, there are many columns of english_score, math_score and number of good deed.\n",
    "#I want to analyze these data and select the best student. But this time the english_score losing over 90% of data,since the column is nearly empty, \n",
    "#it’s better to delete it altogether than risk skewing the analysis.\n",
    "\n",
    "#(3)Applying del df['col'] before df.dropna() can be important because it ensures that dropping columns with excessive missing data \n",
    "#before filtering out rows with just a few missing values. This prevents the potential loss of valuable rows that might only be missing data in the now-deleted column,\n",
    "#thereby maximizing the amount of usable data.\n",
    "\n",
    "#(4)\n",
    "#student_name   math_score   english_score   science_score\n",
    "#Alice          85           90              NaN\n",
    "#Bob            78           NaN             NaN\n",
    "#Carol          92           85              89\n",
    "#Dave           NaN          76              91\n",
    "\n",
    "#Step 1: Analyze Missing Data: The science_score column has 2 missing values,some rows (like Bob's) have multiple missing values.\n",
    "\n",
    "#Step 2: use del df['science_score'] to remove the column because it has too many missing values. \n",
    "#del df['science_score']\n",
    "\n",
    "#Step3: use df.dropna() to remove rows with any remaining missing values.\n",
    "#df = df.dropna()\n",
    "\n",
    "# Reason: Deleting the science_score column because it had too many missing values, making it unreliable for analysis.\n",
    "# Removing rows with missing values: After deleting the science_score column, we used df.dropna() to remove rows with any other \n",
    "# missing values (e.g., rows with missing math_score or history_score). This ensures that the final dataset is complete \n",
    "# and ready for analysis without gaps.\n",
    "\n",
    "# Before and after: Before cleaning: We had 4 rows and 3 columns.There were multiple missing values, particularly in the science_score column.\n",
    "# After cleaning: We now have 1 row and 2 columns. The dataset is complete, with no missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6ebc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Dataset before cleaning:\n",
      "  student_name  math_score  english_score  science_score\n",
      "0        Alice        85.0           90.0            NaN\n",
      "1          Bob        78.0            NaN            NaN\n",
      "2        Carol        92.0           85.0           89.0\n",
      "3         Dave         NaN           76.0           91.0\n",
      "\n",
      "Dataset after cleaning:\n",
      "  student_name  math_score  english_score\n",
      "0        Alice        85.0           90.0\n",
      "2        Carol        92.0           85.0\n"
     ]
    }
   ],
   "source": [
    "#7 here is the complete code below:\n",
    "\n",
    "!pip install pandas numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the initial dataset, using np.nan for missing values\n",
    "data = {\n",
    "    'student_name': ['Alice', 'Bob', 'Carol', 'Dave'],\n",
    "    'math_score': [85, 78, 92, np.nan],\n",
    "    'english_score': [90, np.nan, 85, 76],\n",
    "    'science_score': [np.nan, np.nan, 89, 91],\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset before cleaning\n",
    "print(\"Dataset before cleaning:\")\n",
    "print(df)\n",
    "\n",
    "# Delete the 'science_score' column due to too many missing values\n",
    "del df['science_score']\n",
    "\n",
    "# Remove rows with any remaining missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display the dataset after cleaning\n",
    "print(\"\\nDataset after cleaning:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baceba2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "          count       mean        std     min       25%    50%        75%  \\\n",
      "embarked                                                                    \n",
      "C         168.0  59.954144  83.912994  4.0125  13.69795  29.70  78.500025   \n",
      "Q          77.0  13.276030  14.188047  6.7500   7.75000   7.75  15.500000   \n",
      "S         644.0  27.079812  35.887993  0.0000   8.05000  13.00  27.900000   \n",
      "\n",
      "               max  \n",
      "embarked            \n",
      "C         512.3292  \n",
      "Q          90.0000  \n",
      "S         263.0000  \n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "          count       mean        std     min       25%    50%        75%  \\\n",
      "embarked                                                                    \n",
      "C         168.0  59.954144  83.912994  4.0125  13.69795  29.70  78.500025   \n",
      "Q          77.0  13.276030  14.188047  6.7500   7.75000   7.75  15.500000   \n",
      "S         644.0  27.079812  35.887993  0.0000   8.05000  13.00  27.900000   \n",
      "\n",
      "               max  \n",
      "embarked            \n",
      "C         512.3292  \n",
      "Q          90.0000  \n",
      "S         263.0000  \n",
      "          count       mean        std     min       25%    50%        75%  \\\n",
      "embarked                                                                    \n",
      "C         168.0  59.954144  83.912994  4.0125  13.69795  29.70  78.500025   \n",
      "Q          77.0  13.276030  14.188047  6.7500   7.75000   7.75  15.500000   \n",
      "S         644.0  27.079812  35.887993  0.0000   8.05000  13.00  27.900000   \n",
      "\n",
      "               max  \n",
      "embarked            \n",
      "C         512.3292  \n",
      "Q          90.0000  \n",
      "S         263.0000  \n"
     ]
    }
   ],
   "source": [
    "#8 \n",
    "#（1）df.groupby(\"col1\"): Groups the DataFrame df by unique values in the column col1. \n",
    "#This means all rows with the same value in col1 are put into the same group\n",
    "#[\"col2\"]: From each group created, this selects the col2 column.\n",
    "\n",
    "!pip install pandas numpy\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "embarked_fare_summary = df.groupby('embarked')['fare'].describe()\n",
    "print(embarked_fare_summary)\n",
    "\n",
    "#explain: df.groupby('embarked'): Groups the data by the embarked column \n",
    "#(which indicates the port of embarkation: C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "#['fare']: Focuses on the fare column (ticket price) for analysis.\n",
    "#.describe(): Provides summary statistics of the fare column for each embarkation port group, showing the distribution of ticket prices.\n",
    "\n",
    "#（2）Scope of Analysis:df.describe(): Provides a summary of statistics for each column in the entire DataFrame. \n",
    "#The count here represents the total number of non-missing values for each column across the whole dataset. \n",
    "#This gives a global view of data availability and distribution.df.groupby(\"col1\")[\"col2\"].describe(): Provides statistics for col2 within each group defined by col1. \n",
    "#The count in this context represents the number of non-missing values in col2 for each distinct value of col1. This gives a detailed view of data within specific groups.\n",
    "#Handling of Missing Values: df.describe(): The count reflects the overall impact of missing values on each column in the entire DataFrame.\n",
    "#Missing values affect the count, mean, and other statistics across the whole dataset.df.groupby(\"col1\")[\"col2\"].describe(): The count is specific to each group,\n",
    "#so it shows how missing values are distributed within each group rather than the entire dataset. Different groups may have different numbers of missing values,\n",
    "#which affects the count and other statistics for col2 within each group\n",
    "\n",
    "#（3）Here are the solutions that chatgpt offers to me when I intentionally introduce the following errors to my code.\n",
    "# And also my opnions are below:\n",
    "# In my opinion, the chatgpt is more helpful to solve the problems, if I try to fix those errors using google, I have to search a lot and learn from the websites one by one. \n",
    "# But chagpt collect all suggestions and teach me through a simple way. It is more easier to understand and more quickly to get resolutions.\n",
    "\n",
    "# A：\n",
    "#Fix the Error: To fix this, simply add the missing import statement at the beginning of your code:\n",
    "import pandas as pd\n",
    "\n",
    "# B：\n",
    "#Correct the Typo: correct the typo in the file name to 'titanic.csv'.\n",
    "!pip install pandas \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Corrected URL for the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv' \n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'embarked' and describe the 'fare' column\n",
    "embarked_fare_summary = df.groupby('embarked')['fare'].describe()\n",
    "\n",
    "# Print the result\n",
    "print(embarked_fare_summary)\n",
    "\n",
    "\n",
    "#C:\n",
    "#Correct the Variable Name: ensure using the correct variable name. In this case, change DF to df.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Correct variable name\n",
    "grouped_summary = df.groupby(\"embarked\")[\"fare\"].describe()\n",
    "print(grouped_summary)\n",
    "\n",
    "#D\n",
    "#The issue in the latest version of your code is that the pd.read_csv() function is missing a closing parenthesis ).\n",
    "#If code should be pd.read_csv(url) but you accidentally write pd.read_csv(url, Python will raise a SyntaxError.\n",
    "#This is because the closing parenthesis is missing.\n",
    "#Key Fix:Added the missing closing parenthesis in df = pd.read_csv(url).\n",
    "\n",
    "\n",
    "#E\n",
    "#Mistyping a Function Name:\n",
    "#If code should be df.groupby(\"col1\")[\"col2\"].describe(), but you mistakenly write df.group_by(\"col1\")[\"col2\"].describe(), \n",
    "#Python will raise an AttributeError because group_by is not a valid method for a DataFrame; the correct method is groupby.\n",
    "#Key Fix: Corrected group_by to groupby in df.groupby('embarked')['fare'].describe()\n",
    "#Mistyping a Method Name:\n",
    "#Similarly, if code should be df.groupby(\"col1\")[\"col2\"].describe(), but you mistakenly write df.groupby(\"col1\")[\"col2\"].describle(),\n",
    "#Python will raise an AttributeError because describle is not a valid method; the correct method is describe.\n",
    "\n",
    "\n",
    "\n",
    "#F\n",
    "#It looks like there's a case sensitivity issue with the column name Embarked. \n",
    "#In the Titanic dataset, the column names are typically in lowercase. Let’s ensure we’re using the correct column name.\n",
    "#Key Steps: Print the Column Names: print(df.columns) will show the exact column names and their cases.\n",
    "#Use the Correct Column Name: Make sure to use the exact column name as printed. For the Titanic dataset, it is likely 'embarked' in lowercase.\n",
    "\n",
    "\n",
    "#G\n",
    "#It seems there’s an issue with the embarked variable in your groupby function call.\n",
    "#You should use the column name as a string in the groupby() function.\n",
    "#Key Fix: Use the column name as a string: Replace embarked with 'embarked' in the groupby() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 YES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
